{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Deep Convolutional GAN (DCGAN) and Image Reconstruction using MNIST</h1>\n",
    "\n",
    "<h1>Authors</h1>\n",
    "\n",
    "<ul>\n",
    "    <li>Davide Anghileri</li>\n",
    "    <li>Denis Dushi</li>\n",
    "    <li>Nathan Consuegra</li>\n",
    "</ul>\n",
    "\n",
    "<h1>Introduction</h1>\n",
    "\n",
    "<p>There are 2 main goals for this project:\n",
    "<ul>\n",
    "    <li>The first one is to implement a Deep Convolutional Generative Adversarial Network (DCGAN) that is composed by the following two modules: 1) a discriminator to distinguish between real and fake images, which is a binary Convolutional Neural Network classifier and 2) a generator, which is a Deconvolutional Neural Network, that starting from 100 random numbers between 0 and 1 is able to generate a new picture that is similar to the ones in the dataset in order to fool the discriminator. (See original paper [1]).</li>\n",
    "    <br/>\n",
    "    <li>The second goal, as done in [4], is to be able to reconstruct an image with missing pixels, a task for which we use the generator previously described by giving it as input an array of 100 random pixels. After this, we use Adam Optimization to adjust the input for producing an image able to reconstruct the missing pixels. (See original paper [4]).</li>\n",
    "</ul>\n",
    "<br/>\n",
    "\n",
    "<h1>Architecture</h1>\n",
    "\n",
    "<br/>\n",
    "\n",
    "![Image7-Monitor.png](../../images/network.jpg)\n",
    " The network architecture for the Generator (G) and the Descriminator (D)\n",
    "\n",
    "<br/>\n",
    "<p>The following are the training details:\n",
    "\n",
    "<ul>\n",
    "    <li>Batch size: 100</li>\n",
    "    <li>Learning rate: 0.0002</li>\n",
    "    <li>Training epoch: 20</li>\n",
    "    <li>Leaky ReLU: 0.2</li>\n",
    "    <li>Adam optimizer:</li>\n",
    "        <ul>\n",
    "            <li>beta1: 0.5</li>\n",
    "        </ul>\n",
    "    <li>Dataset normalization:</li>\n",
    "        <ul>\n",
    "            <li>(range: -1 ~ 1)</li>\n",
    "            <li>(pix_val - 0.5) / 0.5</li>\n",
    "        </ul>\n",
    "    <li>Weight init:</li>\n",
    "        <ul>\n",
    "            <li>Normal distribution</li>\n",
    "        </ul>\n",
    "</ul>\n",
    "\n",
    "The following code is adapted from [2] and [3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper(learning_rate = 0.0002, seed = 1234, beta1 = 0.5, batch_size = 100, maskType = 'center'):\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    \n",
    "    # Use this module to get the TensorBoard logdir\n",
    "    from hops import tensorboard\n",
    "    tensorboard_logdir = tensorboard.logdir()\n",
    "    \n",
    "    # use this module to get the project path in hdfs\n",
    "    from hops import hdfs\n",
    "    project_path = hdfs.project_path()\n",
    "\n",
    "    import datetime #logging the time for model checkpoints and training\n",
    "    \n",
    "    # set the parameters\n",
    "    batch_size = batch_size\n",
    "    lr = learning_rate\n",
    "    seed = seed\n",
    "    beta1 = beta1\n",
    "    maskType = maskType\n",
    "    train_epoch = 2\n",
    "    image_size = 64\n",
    "    image_shape = [image_size,image_size,1]\n",
    "    lam = 0.2\n",
    "    centerScale = 0.3\n",
    "    \n",
    "\n",
    "    # Step 1 - Collect dataset\n",
    "    # MNIST - handwritten character digits ~50K training and validation images + labels, 10K testing.\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    # will ensure that the correct data has been downloaded to your \n",
    "    # local training folder and then unpack that data to return a dictionary of DataSet instances.\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True, reshape=[])\n",
    "    \n",
    "    # to have deterministic results\n",
    "    tf.set_random_seed(seed)\n",
    "    \n",
    "    ########-----------------------------------DISCRIMINATOR --------------------------------------------------########\n",
    "    # Take a [batch_size, 64, 64, channels] tensor and output a probability that each image is real (and not fake).\n",
    "    # LRelu is used and it work well for higher resolution modeling.\n",
    "    \n",
    "    def discriminator(x, isTrain=True, reuse=False):\n",
    "        with tf.variable_scope('discriminator', reuse=reuse):\n",
    "            \n",
    "            # 1st hidden layer\n",
    "            conv1 = tf.layers.conv2d(x, 128, [4, 4], strides=(2, 2), padding='same')\n",
    "            lrelu1 = lrelu(conv1, 0.2)\n",
    "    \n",
    "            # 2nd hidden layer\n",
    "            conv2 = tf.layers.conv2d(lrelu1, 256, [4, 4], strides=(2, 2), padding='same')\n",
    "            lrelu2 = lrelu(tf.layers.batch_normalization(conv2, training=isTrain), 0.2)\n",
    "    \n",
    "            # 3rd hidden layer\n",
    "            conv3 = tf.layers.conv2d(lrelu2, 512, [4, 4], strides=(2, 2), padding='same')\n",
    "            lrelu3 = lrelu(tf.layers.batch_normalization(conv3, training=isTrain), 0.2)\n",
    "    \n",
    "            # 4th hidden layer\n",
    "            conv4 = tf.layers.conv2d(lrelu3, 1024, [4, 4], strides=(2, 2), padding='same')\n",
    "            lrelu4 = lrelu(tf.layers.batch_normalization(conv4, training=isTrain), 0.2)\n",
    "    \n",
    "            # output layer\n",
    "            conv5 = tf.layers.conv2d(lrelu4, 1, [4, 4], strides=(1, 1), padding='valid')\n",
    "            o = tf.nn.sigmoid(conv5)\n",
    "    \n",
    "            return o, conv5\n",
    "    \n",
    "    ########-------------------------------------GENERATOR ----------------------------------------------------########\n",
    "    # The generator takes a d-dimensional noise vector and upsamples it to become a 64 x 64 image. \n",
    "    # So, it takes random inputs, and eventually mapping them down to a [1,64,64,channels] pixel.\n",
    "    # We replaces deterministic spatial pooling functions (such as maxpooling) with strided convolutions, \n",
    "    # allowing the network to learn its own spatial downsampling.\n",
    "    # LReLUs are then used to stabilize the outputs of each layer.\n",
    "    # Batch Normalization is used to stabilizes learning and help gradient flows.\n",
    "    \n",
    "    def lrelu(x, th=0.2):\n",
    "        return tf.maximum(th * x, x)\n",
    "\n",
    "    # G(z)\n",
    "    def generator(x, isTrain=True, reuse=False):\n",
    "        with tf.variable_scope('generator', reuse=reuse):\n",
    "    \n",
    "            # 1st hidden layer\n",
    "            conv1 = tf.layers.conv2d_transpose(x, 1024, [4, 4], strides=(1, 1), padding='valid')\n",
    "            lrelu1 = lrelu(tf.layers.batch_normalization(conv1, training=isTrain), 0.2)\n",
    "    \n",
    "            # 2nd hidden layer\n",
    "            conv2 = tf.layers.conv2d_transpose(lrelu1, 512, [4, 4], strides=(2, 2), padding='same')\n",
    "            lrelu2 = lrelu(tf.layers.batch_normalization(conv2, training=isTrain), 0.2)\n",
    "    \n",
    "            # 3rd hidden layer\n",
    "            conv3 = tf.layers.conv2d_transpose(lrelu2, 256, [4, 4], strides=(2, 2), padding='same')\n",
    "            lrelu3 = lrelu(tf.layers.batch_normalization(conv3, training=isTrain), 0.2)\n",
    "    \n",
    "            # 4th hidden layer\n",
    "            conv4 = tf.layers.conv2d_transpose(lrelu3, 128, [4, 4], strides=(2, 2), padding='same')\n",
    "            lrelu4 = lrelu(tf.layers.batch_normalization(conv4, training=isTrain), 0.2)\n",
    "    \n",
    "            # output layer\n",
    "            conv5 = tf.layers.conv2d_transpose(lrelu4, 1, [4, 4], strides=(2, 2), padding='same')\n",
    "            o = tf.nn.tanh(conv5)\n",
    "    \n",
    "            return o\n",
    "    \n",
    "    ########------------------------------------- TRAINING ----------------------------------------------------########\n",
    "\n",
    "    sess = tf.Session() \n",
    "\n",
    "    # fixed z to generate images to see progress in the tensorboard\n",
    "    fixed_z_ = np.random.normal(0, 1, (batch_size, 1, 1, 100))\n",
    "    \n",
    "    # x is the real image input tensor, z the random genereted input tensor\n",
    "    x = tf.placeholder(tf.float32, shape=(None, 64, 64, 1))\n",
    "    z = tf.placeholder(tf.float32, shape=(None, 1, 1, 100))\n",
    "    mask = tf.placeholder(tf.float32, shape=(64, 64, 1), name='mask')\n",
    "    isTrain = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # networks : generator\n",
    "    G_z = generator(z, isTrain)\n",
    "    \n",
    "    G_imgs = generator(z, isTrain, True)\n",
    "\n",
    "    # networks : discriminator\n",
    "    D_real, D_real_logits = discriminator(x, isTrain)\n",
    "    D_fake, D_fake_logits = discriminator(G_z, isTrain, reuse=True) \n",
    "    \n",
    "    # loss for each network\n",
    "    D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones([batch_size, 1, 1, 1])))\n",
    "    D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros([batch_size, 1, 1, 1])))\n",
    "    D_loss = D_loss_real + D_loss_fake\n",
    "    G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones([batch_size, 1, 1, 1])))\n",
    "    \n",
    "    # trainable variables for each network\n",
    "    T_vars = tf.trainable_variables()\n",
    "    D_vars = [var for var in T_vars if var.name.startswith('discriminator')]\n",
    "    G_vars = [var for var in T_vars if var.name.startswith('generator')]\n",
    "    \n",
    "    # Completion.\n",
    " \n",
    "    contextual_loss = tf.reduce_sum(\n",
    "            tf.contrib.layers.flatten(\n",
    "            tf.abs(tf.multiply(mask, G_imgs) - tf.multiply(mask, x))), 1)\n",
    "    perceptual_loss = G_loss\n",
    "    # we want to minimaze the error reconstruction on the not masked part but also the discrimination of the generated image\n",
    "    complete_loss = contextual_loss + lam*perceptual_loss\n",
    "    # compute simbolic derivatives of the complete_loss with respect to z\n",
    "    grad_complete_loss = tf.gradients(complete_loss, z)\n",
    "    \n",
    "    # optimizer for each network\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        D_optim = tf.train.AdamOptimizer(lr, beta1=beta1).minimize(D_loss, var_list=D_vars)\n",
    "        G_optim = tf.train.AdamOptimizer(lr, beta1=beta1).minimize(G_loss, var_list=G_vars)\n",
    "        \n",
    "    #Outputs a Summary containing a single scalar value.\n",
    "    tf.summary.scalar('Generator_loss', G_loss)\n",
    "    tf.summary.scalar('Discriminator_loss_real', D_loss_real)\n",
    "    tf.summary.scalar('Discriminator_loss_fake', D_loss_fake)\n",
    "    tf.summary.scalar('Discriminator_loss', D_loss)\n",
    "        \n",
    "\n",
    "    \n",
    "    #Output 25 generated images using fixed_z_\n",
    "    images_for_tensorboard = generator(z, isTrain, True)\n",
    "    tf.summary.image('Generated_images', images_for_tensorboard, 15)\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # summary for the completion part\n",
    "    tf.summary.scalar('Complete_loss', tf.reduce_mean(complete_loss))\n",
    "    tf.summary.image('before', x, 15)\n",
    "    masked_images = np.multiply(x, mask)\n",
    "    tf.summary.image('masked', masked_images, 15)\n",
    "    # hats is equal to the previous generated images so it can be avoided\n",
    "    tf.summary.image('hats', G_imgs, 15)\n",
    "    inv_masked_hat_images = np.multiply(G_imgs, 1.0-mask)\n",
    "    completed = masked_images + inv_masked_hat_images\n",
    "    tf.summary.image('completed', completed, 15)\n",
    "    merged2 = tf.summary.merge_all()\n",
    "        \n",
    "    writer = tf.summary.FileWriter(tensorboard_logdir, sess.graph)\n",
    "    \n",
    "    # create the saver in order to save the model\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "     \n",
    "    # MNIST resize and normalization\n",
    "    train_set = tf.image.resize_images(mnist.train.images, [64, 64]).eval(session=sess)\n",
    "    train_set = (train_set - 0.5) / 0.5  # normalization; range: -1 ~ 1\n",
    "    val_set = tf.image.resize_images(mnist.validation.images, [64, 64]).eval(session=sess)\n",
    "    val_set = (val_set - 0.5) / 0.5  # normalization; range: -1 ~ 1\n",
    "\n",
    "    #During every iteration, there will be two updates being made, one to the discriminator and one to the generator. \n",
    "    #For the generator update, we’ll feed in a random z vector to the generator and pass that output to the discriminator\n",
    "    #to obtain a probability score.\n",
    "    #As we remember from our loss function, the cross entropy loss gets minimized, \n",
    "    #and only the generator’s weights and biases get updated.\n",
    "    #We'll do the same for the discriminator update. We’ll be taking a batch of real MNIST images.\n",
    "    #These will serve as the positive examples, while the images in the previous section are the negative ones.\n",
    "    \n",
    "    for epoch in range(train_epoch):\n",
    "        \n",
    "        for i in range(mnist.train.num_examples // batch_size):\n",
    "            \n",
    "            # update discriminator\n",
    "            x_ = train_set[i*batch_size:(i+1)*batch_size]\n",
    "            z_ = np.random.normal(0, 1, (batch_size, 1, 1, 100))\n",
    "            loss_d_, _ = sess.run([D_loss, D_optim], {x: x_, z: z_, isTrain: True})\n",
    "    \n",
    "            # update generator\n",
    "            z_ = np.random.normal(0, 1, (batch_size, 1, 1, 100))\n",
    "            loss_g_, _ = sess.run([G_loss, G_optim], {z: z_, x: x_, isTrain: True})\n",
    "    \n",
    "            # every 100 iteration save the model and the summury on the tensorboard\n",
    "            if np.mod(i,100)==0:\n",
    "                #save_path = saver.save(sess, hdfs.project_path()+\"Resources/pretrained_gan\", global_step=i + epoch*(mnist.train.num_examples // batch_size))\n",
    "                x_ = val_set[0:batch_size]\n",
    "                summary = sess.run(merged, {x: x_, z: fixed_z_, isTrain: False})\n",
    "                writer.add_summary(summary, i + epoch*(mnist.train.num_examples // batch_size))\n",
    "                \n",
    "    #START COMPLETION PART\n",
    "\n",
    "    # number of images (better to use a validation set)\n",
    "    nImgs = mnist.train.num_examples\n",
    "    \n",
    "    batch_idxs = int(np.ceil(nImgs/batch_size))\n",
    "    batch_idxs = 1 #only for having some samples and stop, no need to reconstruct all images\n",
    "    \n",
    "    # define the type of mask to apply\n",
    "    if maskType == 'random':\n",
    "        fraction_masked = 0.3\n",
    "        mask1 = np.ones(image_shape).astype(np.float32)\n",
    "        mask1[np.random.random(image_shape[:2]) < fraction_masked] = 0.0\n",
    "        mask1=mask1.astype(np.float32)\n",
    "    elif maskType == 'center':\n",
    "        mask1 = np.ones(image_shape).astype(np.float32)\n",
    "        sz = image_size\n",
    "        l = int(image_size*centerScale)\n",
    "        u = int(image_size*(1.0-centerScale))\n",
    "        mask1[l:u, l:u, :] = 0.0\n",
    "        mask1=mask1.astype(np.float32)\n",
    "    elif maskType == 'left':\n",
    "        mask1 = np.ones(image_shape).astype(np.float32)\n",
    "        c = image_size // 2\n",
    "        mask1[:,:c,:] = 0.0\n",
    "        mask1=mask1.astype(np.float32)\n",
    "    elif maskType == 'grid':\n",
    "        mask1 = np.zeros(image_shape).astype(np.float32)\n",
    "        mask1[::4,::4,:] = 1.0\n",
    "        mask1=mask1.astype(np.float32)\n",
    "    else:\n",
    "        assert(False)\n",
    "\n",
    "    #for each batch\n",
    "    for idx in range(0, batch_idxs):\n",
    "        l = idx*batch_size\n",
    "        u = min((idx+1)*batch_size, nImgs)\n",
    "        batchSz = u-l\n",
    "        batch_images = train_set[l:u]\n",
    "        batch_images = np.array(batch_images).astype(np.float32)\n",
    "    \n",
    "        #shouldn't occur since 55000 is multiple of 100, but present for safety\n",
    "        if batchSz < batch_size:\n",
    "            padSz = ((0, int(batch_size-batchSz)), (0,0), (0,0), (0,0))\n",
    "            batch_images = np.pad(batch_images, padSz, 'constant').astype(np.float32)\n",
    "\n",
    "        # starts with 100 random numbers\n",
    "        zhats = np.random.normal(0, 1, (batch_size, 1, 1, 100)).astype(np.float32)\n",
    "        m = 0\n",
    "        v = 0\n",
    "        beta2 = beta1\n",
    "        nIter = 10000 # number of iteration of the completion optimization for each batch\n",
    "\n",
    "        for i in range(nIter):\n",
    "            fd = {\n",
    "                z: zhats,\n",
    "                mask: mask1,\n",
    "                x: batch_images,\n",
    "                isTrain: False\n",
    "            }\n",
    "            run = [complete_loss, grad_complete_loss]\n",
    "            loss, g = sess.run(run, feed_dict=fd)\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                # save the images before, masked, hats (generated) and completed.\n",
    "                summary2 = sess.run(merged2, {z: zhats, mask: mask1, x: batch_images, isTrain: False})\n",
    "                writer.add_summary(summary2, i+(idx*nIter) )\n",
    "                             \n",
    "            # Optimize single completion with Adam\n",
    "            m_prev = np.copy(m)\n",
    "            v_prev = np.copy(v)\n",
    "            m = beta1 * m_prev + (1 - beta1) * g[0]\n",
    "            v = beta2 * v_prev + (1 - beta2) * np.multiply(g[0], g[0])\n",
    "            m_hat = m / (1 - beta1 ** (i + 1))\n",
    "            v_hat = v / (1 - beta2 ** (i + 1))\n",
    "            zhats += - np.true_divide(lr * m_hat, (np.sqrt(v_hat) + 0.000000001))\n",
    "            #maybe change the interval to 0,1 as the input fixed_z_\n",
    "            zhats = np.clip(zhats, -1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import util\n",
    "\n",
    "#Define dict for hyperparameters\n",
    "args_dict = {'learning_rate': [ 0.0002], 'seed' : [5678], 'beta1' : [0.5], 'batch_size' : [100], 'maskType' : ['left', 'grid']}\n",
    "\n",
    "# Generate a grid for the given hyperparameters\n",
    "args_dict_grid = util.grid_params(args_dict)\n",
    "\n",
    "print(args_dict_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import experiment\n",
    "\n",
    "tensorboard_hdfs_logdir = experiment.launch(spark, wrapper, args_dict_grid, name='mnist gan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import tensorboard\n",
    "\n",
    "tensorboard.visualize(spark, tensorboard_hdfs_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Results</h1>\n",
    "\n",
    "<h3>Image generation using DCGAN</h3>\n",
    "\n",
    "Random images generated after 20 epochs.\n",
    "![Image7-Monitor.png](../../images/final_grid.jpg)\n",
    "<p>After training the algorithm for <b>20 epochs</b> the generator was able to generate images very similar to our training data. The time required to train this model for 20 epochs is about 3 days on a single CPU and 1 hour on a GPU.\n",
    "<br/>\n",
    "<br/>\n",
    "We can see the <b>generator and the discriminator losses</b> evaluated on the validation set every 100 iteration. (On the y axis we have the loss while on the x axis we have the iteration number, and every 550 iteration correspond to 1 epoch since we use a batch size of 100 and the training dataset consists of 55000 images).<br/><br/>\n",
    "    \n",
    "![Image7-Monitor.png](../../images/DCGAN_Mnist_tensorboard_scalars_50kIter.jpg)\n",
    "<br/>\n",
    "We can also observe that the generator starts to generate quite good images after few iteration and then it try to continue to improve the images with a lot of <b>oscillations without converging to a fixed solution</b>. We can observe how the generated images evolve at each epoch. (Note that at epoch 7 the generated images are random pixels even if before we collected images similar to numbers and after the generator start again to generate good images). The following image shows the 10 epochs image generation process. <br/><br/>\n",
    "\n",
    "![Image7-Monitor.png](../../images/gif_speed_low.gif)\n",
    "\n",
    "<br/>\n",
    "Finally we did <b>hyperparameter tuning</b> and we obtained the following results:\n",
    "<ul>\n",
    "<li>No clear difference in using a Learning Rate of 0.0005 or 0.0002, probably due to the fact that we used Adam Optimizer that automatically adapt the learning rate for each parameter,</li>\n",
    "<li>The Momentum parameter <i>beta1</i> set to 0.5 helps in reducing the oscillations during training compared to 0.9,</li>\n",
    "<li>A bacth size of 100 images is a good compromise between quality of the solution and time to train for the MNIST dataset.</li>\n",
    "</ul>\n",
    "<br/>\n",
    "These results agree with the results presented in [1].\n",
    "</p>\n",
    "\n",
    "<h3>Image Reconstruction</h3>\n",
    "\n",
    "The objective is <b>to reconstruct an image that contains missing pixels</b>. In order to do that we use the generator of the previous step, we give as input an array of 100 random pixels and finally we do Adam Optimization to adjust the input to produce an image that is able to reconstruct the missing pixels.\n",
    "In the following figure, we can see the results of these reconstruction, in the first column there is the image before the pixels were deleted, in the second column the masked image and in the third the reconstructed image.\n",
    "<br/>\n",
    "\n",
    "![Image7-Monitor.png](../../images/res.jpg)\n",
    "<br/>\n",
    "![Image7-Monitor.png](../../images/center.gif)\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1>References</h1>\n",
    "<ol type=\"1\">\n",
    "  <li>Radford, Alec, Luke Metz, and Soumith Chintala. \"Unsupervised representation learning with deep convolutional \n",
    "    generative adversarial networks.\" arXiv preprint arXiv:1511.06434 (2015). (Full paper: https://arxiv.org/pdf/1511.06434.pdf)</li>\n",
    "<li>Brandon Amos. Image Completion with Deep Learning in TensorFlow. http://bamos.github.io/2016/08/09/deep-completion. Accessed: [08/01/2018]</li>\n",
    "<li>Znxlwm, \"Tensorflow implementation of Generative Adversarial Networks (GAN) and Deep Convolutional Generative Adversarial Netwokrs for MNIST dataset\". https://github.com/znxlwm/tensorflow-MNIST-GAN-DCGAN Accessed: [08/01/2018]</li>\n",
    "  <li>Raymond A. Yeh, Chen Chen, Teck Yian Lim, Alexander G. Schwing, Mark Hasegawa-Johnson, Minh N. Do. \"Semantic Image Inpainting with Deep Generative Models\". arXiv preprint arXiv:1607.07539 (2017). (Full paper: https://arxiv.org/pdf/1607.07539.pdf)</li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
