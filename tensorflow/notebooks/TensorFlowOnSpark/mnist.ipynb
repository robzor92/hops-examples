{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started: Mnist TensorFlowOnSpark on Hops Notebook\n",
    "---\n",
    "\n",
    "<font color='red'> <h3>Before running this notebook, you need to run the [Mnist Preprocessing Notebook](mnist_preprocessing.ipynb)</h3></font>\n",
    "\n",
    "<font color='red'> <h3>Tested with TensorFlow 1.8</h3></font>\n",
    "\n",
    "In this notebook we are going to master running TensorFlowOnSpark on Hops, Jupyter notebooks and HopsFS, the highly efficient distributed file system that Hops provides. HopsFS is a fork of Apache HDFS and operations for reading and writing files to HopsFS is the same as with HDFS that TensorFlow supports.\n",
    "\n",
    "For more information about how to read from HDFS, please see: https://www.tensorflow.org/deploy/hadoop\n",
    "\n",
    "Observe that this example has been extensively modified to support different data formats (CSV and TFRecords), in addition to both training and inference mode.\n",
    "\n",
    "In this example program we are going to:\n",
    "- Introduce TensorFlowOnSpark and improvements made by Hops\n",
    "- Using the `hops` python library to run TensorFlow on the Hops Platform\n",
    "- Read data from a Dataset which is located in your project, on HopsFS (HDFS)\n",
    "- Train a neural network \n",
    "- Monitor training using TensorBoard to see accuracy and loss\n",
    "- Monitoring training by looking at logs\n",
    "- Visualize TensorBoard events from the previous TensorBoard run\n",
    "\n",
    "## Table of contents:\n",
    "\n",
    "### [TensorFlowOnSpark on Hops](#introducing)\n",
    "### [The TFCluster API](#starting)\n",
    "### [Monitoring execution - TensorBoard](#tensorboard)\n",
    "### [Monitoring execution - Logs](#logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlowOnSpark on Hops <a class=\"anchor\" id='introducing'></a>\n",
    "\n",
    "See the official Github repo: https://github.com/yahoo/TensorFlowOnSpark\n",
    "\n",
    "For the original example: https://github.com/yahoo/TensorFlowOnSpark/tree/master/examples/mnist\n",
    "\n",
    "Originally developed by Yahoo, TensorFlowOnSpark is essentially a wrapper for Distributed TensorFlow (https://www.tensorflow.org/deploy/distributed) and in that sense, TensorFlowOnSpark supports all features which Distributed TensorFlow provides, such as asynchronous or synchronous training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TFCluster API\n",
    "\n",
    "The `TFCluster` python module is used to launch the actual TensorFlowOnSpark program. It is similar to the `experiment` module in the sense that a wrapper function needs to be created that contains all the TensorFlow code.\n",
    "\n",
    "Please see the following links for information on the programming model:\n",
    "\n",
    "Yahoo introduces TensorFlowOnSpark API: https://www.youtube.com/watch?v=b3lTvTKBatE\n",
    "Converting Distributed TensorFlow to TensorFlowOnSpark: https://github.com/yahoo/TensorFlowOnSpark/wiki/Conversion-Guide\n",
    "Documentation: https://yahoo.github.io/TensorFlowOnSpark/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring execution - TensorBoard <a class=\"anchor\" id='tensorboard'></a>\n",
    "To find the TensorBoard for the execution, please go back to HopsWorks and follow the arrows in the images below.\n",
    "\n",
    "![Image7-Monitor.png](../images/jupyter.png)\n",
    "\n",
    "![Image4-LaunchTensorboard.png](../images/overview.png)\n",
    "\n",
    "![Image8-Tensorboard.png](../images/tensorboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring execution - logs <a class=\"anchor\" id='logs'></a>\n",
    "To find the logs for the execution, please go back to HopsWorks and follow the arrows in the images below.\n",
    "\n",
    "![Image6-GoToLogs.png](../images/logs.png)\n",
    "\n",
    "![Image9-Logs.png](../images/viewlogs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mnist TensorFlowOnSpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_fun(args, ctx):\n",
    "    \n",
    "    def print_log(worker_num, arg):\n",
    "        print(\"%d: \" %worker_num)\n",
    "        print(arg)\n",
    "\n",
    "    from tensorflowonspark import TFNode\n",
    "    from datetime import datetime\n",
    "    import getpass\n",
    "    import math\n",
    "    import numpy\n",
    "    import os\n",
    "    import signal\n",
    "    import tensorflow as tf\n",
    "    import time\n",
    "  \n",
    "    # Used to get TensorBoard logdir for TensorBoard that show up in HopsWorks\n",
    "    from hops import tensorboard\n",
    "\n",
    "    IMAGE_PIXELS=28\n",
    "    worker_num = ctx.worker_num\n",
    "    job_name = ctx.job_name\n",
    "    task_index = ctx.task_index\n",
    "    cluster_spec = ctx.cluster_spec\n",
    "    num_workers = len(cluster_spec['worker'])\n",
    "\n",
    "    # Delay PS nodes a bit, since workers seem to reserve GPUs more quickly/reliably (w/o conflict)\n",
    "    if job_name == \"ps\":\n",
    "        time.sleep((worker_num + 1) * 5)\n",
    "\n",
    "    # Parameters\n",
    "    hidden_units = 128\n",
    "    batch_size   = 100\n",
    "\n",
    "    # Get TF cluster and server instances\n",
    "    cluster, server = TFNode.start_cluster_server(ctx, 1, args.rdma)\n",
    "\n",
    "    def read_tfr_examples(path, batch_size=100, num_epochs=None, task_index=None, num_workers=None):\n",
    "        print_log(worker_num, \"num_epochs: {0}\".format(num_epochs))\n",
    "\n",
    "        # Setup queue of TFRecord filenames\n",
    "        tf_record_pattern = os.path.join(path, 'part-*')\n",
    "        files = tf.gfile.Glob(tf_record_pattern)\n",
    "        queue_name = \"file_queue\"\n",
    "\n",
    "        # split input files across workers, if specified\n",
    "        if task_index is not None and num_workers is not None:\n",
    "            num_files = len(files)\n",
    "            files = files[task_index:num_files:num_workers]\n",
    "            queue_name = \"file_queue_{0}\".format(task_index)\n",
    "\n",
    "        print_log(worker_num, \"files: {0}\".format(files))\n",
    "        file_queue = tf.train.string_input_producer(files, shuffle=False, capacity=1000, num_epochs=num_epochs, name=queue_name)\n",
    "\n",
    "        # Setup reader for examples\n",
    "        reader = tf.TFRecordReader(name=\"reader\")\n",
    "        _, serialized = reader.read(file_queue)\n",
    "        feature_def = {'label': tf.FixedLenFeature([10], tf.int64), 'image': tf.FixedLenFeature([784], tf.int64) }\n",
    "        features = tf.parse_single_example(serialized, feature_def)\n",
    "        norm = tf.constant(255, dtype=tf.float32, shape=(784,))\n",
    "        image = tf.div(tf.to_float(features['image']), norm)\n",
    "        print_log(worker_num, \"image: {0}\".format(image))\n",
    "        label = tf.to_float(features['label'])\n",
    "        print_log(worker_num, \"label: {0}\".format(label))\n",
    "\n",
    "        # Return a batch of examples\n",
    "        return tf.train.batch([image,label], batch_size, num_threads=args.readers, name=\"batch\")\n",
    "\n",
    "    def read_csv_examples(image_dir, label_dir, batch_size=100, num_epochs=None, task_index=None, num_workers=None):\n",
    "        print_log(worker_num, \"num_epochs: {0}\".format(num_epochs))\n",
    "        # Setup queue of csv image filenames\n",
    "        tf_record_pattern = os.path.join(image_dir, 'part-*')\n",
    "        images = tf.gfile.Glob(tf_record_pattern)\n",
    "        print_log(worker_num, \"images: {0}\".format(images))\n",
    "        image_queue = tf.train.string_input_producer(images, shuffle=False, capacity=1000, num_epochs=num_epochs, name=\"image_queue\")\n",
    "\n",
    "        # Setup queue of csv label filenames\n",
    "        tf_record_pattern = os.path.join(label_dir, 'part-*')\n",
    "        labels = tf.gfile.Glob(tf_record_pattern)\n",
    "        print_log(worker_num, \"labels: {0}\".format(labels))\n",
    "        label_queue = tf.train.string_input_producer(labels, shuffle=False, capacity=1000, num_epochs=num_epochs, name=\"label_queue\")\n",
    "\n",
    "        # Setup reader for image queue\n",
    "        img_reader = tf.TextLineReader(name=\"img_reader\")\n",
    "        _, img_csv = img_reader.read(image_queue)\n",
    "        image_defaults = [ [1.0] for col in range(784) ]\n",
    "        img = tf.stack(tf.decode_csv(img_csv, image_defaults))\n",
    "        # Normalize values to [0,1]\n",
    "        norm = tf.constant(255, dtype=tf.float32, shape=(784,))\n",
    "        image = tf.div(img, norm)\n",
    "        print_log(worker_num, \"image: {0}\".format(image))\n",
    "\n",
    "        # Setup reader for label queue\n",
    "        label_reader = tf.TextLineReader(name=\"label_reader\")\n",
    "        _, label_csv = label_reader.read(label_queue)\n",
    "        label_defaults = [ [1.0] for col in range(10) ]\n",
    "        label = tf.stack(tf.decode_csv(label_csv, label_defaults))\n",
    "        print_log(worker_num, \"label: {0}\".format(label))\n",
    "\n",
    "        # Return a batch of examples\n",
    "        return tf.train.batch([image,label], batch_size, num_threads=args.readers, name=\"batch_csv\")\n",
    "\n",
    "    if job_name == \"ps\":\n",
    "        server.join()\n",
    "    elif job_name == \"worker\":\n",
    "        # Assigns ops to the local worker by default.\n",
    "        with tf.device(tf.train.replica_device_setter(\n",
    "            worker_device=\"/job:worker/task:%d\" % task_index,\n",
    "            cluster=cluster)):\n",
    "\n",
    "            # Variables of the hidden layer\n",
    "            hid_w = tf.Variable(tf.truncated_normal([IMAGE_PIXELS * IMAGE_PIXELS, hidden_units],\n",
    "                                  stddev=1.0 / IMAGE_PIXELS), name=\"hid_w\")\n",
    "            hid_b = tf.Variable(tf.zeros([hidden_units]), name=\"hid_b\")\n",
    "            tf.summary.histogram(\"hidden_weights\", hid_w)\n",
    "\n",
    "            # Variables of the softmax layer\n",
    "            sm_w = tf.Variable(tf.truncated_normal([hidden_units, 10],\n",
    "                                  stddev=1.0 / math.sqrt(hidden_units)), name=\"sm_w\")\n",
    "            sm_b = tf.Variable(tf.zeros([10]), name=\"sm_b\")\n",
    "            tf.summary.histogram(\"softmax_weights\", sm_w)\n",
    "\n",
    "            # Placeholders or QueueRunner/Readers for input data\n",
    "            num_epochs = 1 if args.mode == \"inference\" else None if args.epochs == 0 else args.epochs\n",
    "            index = task_index if args.mode == \"inference\" else None\n",
    "            workers = num_workers if args.mode == \"inference\" else None\n",
    "\n",
    "            if args.format == \"csv\":\n",
    "                images = TFNode.hdfs_path(ctx, args.images)\n",
    "                labels = TFNode.hdfs_path(ctx, args.labels)\n",
    "                x, y_ = read_csv_examples(images, labels, 100, num_epochs, index, workers)\n",
    "            elif args.format == \"tfr\":\n",
    "                images = TFNode.hdfs_path(ctx, args.images)\n",
    "                x, y_ = read_tfr_examples(images, 100, num_epochs, index, workers)\n",
    "            else:\n",
    "                raise(\"{0} format not supported for tf input mode\".format(args.format))\n",
    "\n",
    "            x_img = tf.reshape(x, [-1, IMAGE_PIXELS, IMAGE_PIXELS, 1])\n",
    "            tf.summary.image(\"x_img\", x_img)\n",
    "\n",
    "            hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\n",
    "            hid = tf.nn.relu(hid_lin)\n",
    "\n",
    "            y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n",
    "\n",
    "            global_step = tf.Variable(0)\n",
    "\n",
    "            loss = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "            train_op = tf.train.AdagradOptimizer(0.01).minimize(\n",
    "              loss, global_step=global_step)\n",
    "\n",
    "            # Test trained model\n",
    "            label = tf.argmax(y_, 1, name=\"label\")\n",
    "            prediction = tf.argmax(y, 1,name=\"prediction\")\n",
    "            correct_prediction = tf.equal(prediction, label)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "            tf.summary.scalar(\"acc\", accuracy)\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            summary_op = tf.summary.merge_all()\n",
    "            init_op = tf.global_variables_initializer()\n",
    "\n",
    "            # Create a \"supervisor\", which oversees the training process and stores model state into HDFS\n",
    "            logdir = tensorboard.logdir()\n",
    "            print(\"tensorflow model path: {0}\".format(logdir))\n",
    "\n",
    "            if job_name == \"worker\" and task_index == 0:\n",
    "                summary_writer = tf.summary.FileWriter(logdir, graph=tf.get_default_graph())\n",
    "\n",
    "            if args.mode == \"train\":\n",
    "                sv = tf.train.Supervisor(is_chief=(task_index == 0),\n",
    "                                       logdir=logdir,\n",
    "                                       init_op=init_op,\n",
    "                                       summary_op=None,\n",
    "                                       summary_writer=None,\n",
    "                                       saver=saver,\n",
    "                                       global_step=global_step,\n",
    "                                       stop_grace_secs=300,\n",
    "                                       save_model_secs=10)\n",
    "            else:\n",
    "                sv = tf.train.Supervisor(is_chief=(task_index == 0),\n",
    "                                       logdir=logdir,\n",
    "                                       summary_op=None,\n",
    "                                       saver=saver,\n",
    "                                       global_step=global_step,\n",
    "                                       stop_grace_secs=300,\n",
    "                                       save_model_secs=0)\n",
    "            output_dir = TFNode.hdfs_path(ctx, args.output)\n",
    "            output_file = tf.gfile.Open(\"{0}/part-{1:05d}\".format(output_dir, worker_num), mode='w')\n",
    "\n",
    "          # The supervisor takes care of session initialization, restoring from\n",
    "          # a checkpoint, and closing when done or an error occurs.\n",
    "    with sv.managed_session(server.target) as sess:\n",
    "        print(\"{0} session ready\".format(datetime.now().isoformat()))\n",
    "\n",
    "        # Loop until the supervisor shuts down or 1000000 steps have completed.\n",
    "        step = 0\n",
    "        count = 0\n",
    "        while not sv.should_stop() and step < args.steps:\n",
    "        # Run a training step asynchronously.\n",
    "        # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n",
    "        # perform *synchronous* training.\n",
    "\n",
    "            # using QueueRunners/Readers\n",
    "            if args.mode == \"train\":\n",
    "                if (step % 100 == 0):\n",
    "                    print(\"{0} step: {1} accuracy: {2}\".format(datetime.now().isoformat(), step, sess.run(accuracy)))\n",
    "                _, summary, step = sess.run([train_op, summary_op, global_step])\n",
    "                if sv.is_chief:\n",
    "                    summary_writer.add_summary(summary, step)\n",
    "            else: # args.mode == \"inference\"\n",
    "                labels, pred, acc = sess.run([label, prediction, accuracy])\n",
    "                #print(\"label: {0}, pred: {1}\".format(labels, pred))\n",
    "                print(\"acc: {0}\".format(acc))\n",
    "                for i in range(len(labels)):\n",
    "                    count += 1\n",
    "                    output_file.write(\"{0} {1}\\n\".format(labels[i], pred[i]))\n",
    "                print(\"count: {0}\".format(count))\n",
    "\n",
    "        if args.mode == \"inference\":\n",
    "            output_file.close()\n",
    "            # Delay chief worker from shutting down supervisor during inference, since it can load model, start session,\n",
    "            # run inference and request stop before the other workers even start/sync their sessions.\n",
    "        if task_index == 0:\n",
    "            time.sleep(60)\n",
    "\n",
    "        # Ask for all the services to stop.\n",
    "        print(\"{0} stopping supervisor\".format(datetime.now().isoformat()))\n",
    "        sv.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from datetime import datetime\n",
    "from hops import util\n",
    "from hops import hdfs\n",
    "from hops import experiment\n",
    "\n",
    "TFCluster = experiment.TFCluster\n",
    "\n",
    "sc = spark.sparkContext\n",
    "num_executors = util.num_executors(spark)\n",
    "num_ps = util.num_param_servers(spark)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-e\", \"--epochs\", help=\"number of epochs\", type=int, default=0)\n",
    "parser.add_argument(\"-f\", \"--format\", help=\"example format: (csv|pickle|tfr)\", choices=[\"csv\",\"pickle\",\"tfr\"], default=\"csv\")\n",
    "parser.add_argument(\"-i\", \"--images\", help=\"HDFS path to MNIST images in parallelized format\", default='/Projects/' + hdfs.project_name() + '/mnist/train/images')\n",
    "parser.add_argument(\"-l\", \"--labels\", help=\"HDFS path to MNIST labels in parallelized format\", default = '/Projects/' + hdfs.project_name() + '/mnist/train/labels')\n",
    "parser.add_argument(\"-m\", \"--model\", help=\"HDFS path to save/load model during train/test\", default=\"mnist_model\")\n",
    "parser.add_argument(\"-n\", \"--cluster_size\", help=\"number of nodes in the cluster (for Spark Standalone)\", type=int, default=num_executors)\n",
    "parser.add_argument(\"-o\", \"--output\", help=\"HDFS path to save test/inference output\", default=\"predictions\")\n",
    "parser.add_argument(\"-r\", \"--readers\", help=\"number of reader/enqueue threads\", type=int, default=1)\n",
    "parser.add_argument(\"-s\", \"--steps\", help=\"maximum number of steps\", type=int, default=500)\n",
    "parser.add_argument(\"-tb\", \"--tensorboard\", help=\"launch tensorboard process\", action=\"store_true\")\n",
    "parser.add_argument(\"-X\", \"--mode\", help=\"train|inference\", default=\"train\")\n",
    "parser.add_argument(\"-c\", \"--rdma\", help=\"use rdma connection\", default=False)\n",
    "args = parser.parse_args()\n",
    "print(\"args:\",args)\n",
    "\n",
    "\n",
    "print(\"{0} ===== Start\".format(datetime.now().isoformat()))\n",
    "\n",
    "cluster = TFCluster.run(sc, mnist_fun, args, args.cluster_size, num_ps, args.tensorboard, TFCluster.InputMode.TENSORFLOW, name='simple dist mnist', local_logdir=True)\n",
    "cluster.shutdown()\n",
    "\n",
    "print(\"{0} ===== Stop\".format(datetime.now().isoformat()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
